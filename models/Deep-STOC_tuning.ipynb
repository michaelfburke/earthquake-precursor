{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-STOC: Deep Learning Model for Offshore Earthquake Prediction\n",
    "\n",
    "This notebook presents the steps involved in tuning a Convolutional LSTM model (Deep-STOC) used for predicting the likelihood of offshore earthquakes based on remote sensing data. The model is constructed and trained using TensorFlow and Keras libraries in Python. We make use of the Keras Tuner library to perform hyperparameter tuning to optimize the model's performance.\n",
    "\n",
    "The process in this notebook is divided into several steps:\n",
    "\n",
    "1. **Data Loading:** The data used for training and testing the model is loaded from a specific location in the Google Drive.\n",
    "\n",
    "2. **Data Preprocessing:** The loaded data is preprocessed, which includes reshaping and normalizing the data using MinMaxScaler from sklearn.\n",
    "\n",
    "3. **Train-Test Split:** The preprocessed data is divided into training and testing datasets.\n",
    "\n",
    "4. **Hypermodel Definition:** A custom HyperModel is defined using the Keras Tuner's HyperModel class. In the HyperModel, we build a ConvLSTM model with several layers where each layer's hyperparameters (like the number of filters, kernel size, activation function) are variables.\n",
    "\n",
    "5. **Hyperparameter Tuning:** We perform hyperparameter tuning using Bayesian Optimization, searching through the hyperparameters' predefined space to find the set that results in the best validation accuracy.\n",
    "\n",
    "6. **Model Evaluation:** The best hyperparameters and model are fetched, and the model summary is printed.\n",
    "\n",
    "7. **Trial Details:** All the trials conducted by the tuner are displayed with their respective hyperparameters and scores.\n",
    "\n",
    "The ultimate goal of this notebook is to find the most effective model (best hyperparameters) for predicting offshore earthquakes using remote sensing data.\n",
    "\n",
    "Please ensure the correct versions of all required libraries are installed and that the data file paths are set correctly for successful execution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras_tuner import HyperModel, RandomSearch, BayesianOptimization\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, ConvLSTM2D, Flatten\n",
    "\n",
    "# Mount google drive for data loading\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load data from drive\n",
    "X = np.load('drive/MyDrive/X_deepcolour7.npy', allow_pickle = True)\n",
    "y = np.load('drive/MyDrive/y_deepcolour7.npy', allow_pickle = True)\n",
    "\n",
    "# Preprocess loaded data\n",
    "values = np.stack([sample[0] for sample in X])\n",
    "X = values.transpose(0, 1, 2, 3, 4) \n",
    "\n",
    "# Display data shape\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a separate MinMaxScaler for each variable (channel)\n",
    "scalers = [MinMaxScaler() for _ in range(X_train.shape[-1])]\n",
    "\n",
    "# Fit the scalers to the training data and transform the training and testing data\n",
    "for i in range(X_train.shape[-1]):\n",
    "    X_train[..., i] = scalers[i].fit_transform(X_train[..., i].reshape(-1, 1)).reshape(X_train[..., i].shape)\n",
    "    X_test[..., i] = scalers[i].transform(X_test[..., i].reshape(-1, 1)).reshape(X_test[..., i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the HyperModel class for hyperparameter tuning\n",
    "class MyHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "\n",
    "    def build(self, hp):\n",
    "        # Build the model with hyperparameters\n",
    "        # hp.Int and hp.Choice are used to specify ranges for the hyperparameters\n",
    "        \n",
    "        model = Sequential()\n",
    "        # Define first layer with variable hyperparameters\n",
    "        model.add(ConvLSTM2D(\n",
    "            filters=hp.Int('filters_1', 32, 128, step=32),\n",
    "            kernel_size=(hp.Int('kernel_size_1_height', 2, 4), hp.Int('kernel_size_1_width', 2, 4)),\n",
    "            activation=hp.Choice('activation_1', ['relu', 'tanh']),\n",
    "            return_sequences=True,\n",
    "            input_shape=self.input_shape\n",
    "        ))\n",
    "        \n",
    "        # Add additional ConvLSTM2D layers based on the hyperparameter 'additional_layers'\n",
    "        additional_layers = hp.Int('additional_layers', 1, 3)\n",
    "        for i in range(additional_layers):\n",
    "            model.add(ConvLSTM2D(\n",
    "                filters=hp.Int(f'filters_{i+2}', 32, 128, step=32),\n",
    "                kernel_size=(hp.Int(f'kernel_size_{i+2}_height', 2, 4), hp.Int(f'kernel_size_{i+2}_width', 2, 4)),\n",
    "                activation=hp.Choice(f'activation_{i+2}', ['relu', 'tanh']),\n",
    "                return_sequences=True if i < additional_layers - 1 else False\n",
    "            ))\n",
    "\n",
    "        # Flatten layer\n",
    "        model.add(Flatten())\n",
    "        # Dropout layer with variable rate\n",
    "        model.add(Dropout(rate=hp.Float('dropout_rate', 0, 0.5, step=0.1)))\n",
    "        # Final Dense layer\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "        # Compile the model with variable optimizer\n",
    "        model.compile(\n",
    "            optimizer=hp.Choice('optimizer', ['adam', 'rmsprop']),\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "\n",
    "# Initialize HyperModel with the input shape\n",
    "hypermodel = MyHyperModel(input_shape=(None, X_train.shape[2], X_train.shape[3], X_train.shape[4]))\n",
    "\n",
    "# Initialize BayesianOptimization tuner\n",
    "tuner = BayesianOptimization(\n",
    "    hypermodel,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,\n",
    "    num_initial_points=2,  # number of randomly selected hyperparameter configurations to test\n",
    "    seed=43,  # to ensure reproducibility\n",
    "    directory='my_dir',\n",
    "    project_name='convlstm_bayesian'\n",
    ")\n",
    "\n",
    "# Execute search with training data\n",
    "tuner.search(X_train, y_train, epochs=50, validation_data=(X_test, y_test), batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch the best hyperparameters and best model\n",
    "best_hyperparameters = tuner.get_best_hyperparameters()[0]\n",
    "best_model = tuner.get_best_models()[0]\n",
    "\n",
    "# Print the best hyperparameters and the model summary\n",
    "print(\"Best Hyperparameters: \", best_hyperparameters.values)\n",
    "print(best_model.summary())\n",
    "\n",
    "# Fetch all the trial details\n",
    "all_trials = tuner.oracle.trials\n",
    "\n",
    "# Print each trial's hyperparameters and score\n",
    "for trial_id, trial in all_trials.items():\n",
    "    print(\"Trial \", trial_id)\n",
    "    print(\"Hyperparameters: \", trial.hyperparameters.values)\n",
    "    print(\"Score: \", trial.score)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
