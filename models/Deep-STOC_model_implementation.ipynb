{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep-STOC Model Implementation for Offshore Earthquake Prediction\n",
    "This notebook showcases the final implementation of the Deep-STOC (Deep Learning for Spatio-Temporal Ocean Colour) model. The model was developed as part of my MSc Dissertation, which aims to predict the likelihood of offshore earthquakes using remote sensing data.\n",
    "\n",
    "The Deep-STOC model is a Convolutional LSTM network trained and tested on remote sensing data, specifically designed to understand the spatio-temporal characteristics of seismic activities in the offshore regions. The parameters used for this model were derived from a detailed hyperparameter tuning process, the code for which can be found in the Deep-STOC_tuning.ipynb notebook.\n",
    "\n",
    "In this notebook, we:\n",
    "\n",
    "1. **Load and preprocess** the seismic data.\n",
    "2. **Normalize** the data using the MinMaxScaler from the sklearn library.\n",
    "3. **Define the architecture** of the Deep-STOC model using parameters identified from hyperparameter tuning.\n",
    "4. **Train** the model and make predictions on the test data.\n",
    "5. **Evaluate** the performance of the model using various metrics, including accuracy, precision, recall, F1 score, and ROC AUC.\n",
    "6. Plot the **ROC curve** to visualize the model's performance.\n",
    "7. Analyze **feature importance** via permutation importance, providing an estimate of the contribution of each feature to the model's prediction.\n",
    "8. Plot the **Partial Dependence Plots (PDPs)** for selected features, offering a way to visualize the effect of certain features on the output prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import drive\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_auc_score, log_loss, roc_curve, auc\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import ConvLSTM2D, Flatten, Dropout, Dense\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Mount Google Drive for data access\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load the data from Google Drive\n",
    "X = np.load('drive/MyDrive/X_deepcolour7.npy', allow_pickle = True)\n",
    "y = np.load('drive/MyDrive/y_deepcolour7.npy', allow_pickle = True)\n",
    "\n",
    "# Preprocess data: stack and transpose dimensions\n",
    "values = np.stack([sample[0] for sample in X])\n",
    "X = values.transpose(0, 1, 2, 3, 4)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize each channel separately\n",
    "scalers = [MinMaxScaler() for _ in range(X_train.shape[-1])]\n",
    "for i in range(X_train.shape[-1]):\n",
    "    X_train[..., i] = scalers[i].fit_transform(X_train[..., i].reshape(-1, 1)).reshape(X_train[..., i].shape)\n",
    "    X_test[..., i] = scalers[i].transform(X_test[..., i].reshape(-1, 1)).reshape(X_test[..., i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "# Based on hyperparameter tuning results, we define a ConvLSTM model with specific parameters\n",
    "model = Sequential()\n",
    "\n",
    "# Initial ConvLSTM2D layer\n",
    "model.add(ConvLSTM2D(\n",
    "    filters=128,  \n",
    "    kernel_size=(4, 3),  \n",
    "    activation='tanh',  \n",
    "    return_sequences=True,\n",
    "    input_shape=(None, X_train.shape[2], X_train.shape[3], X_train.shape[4])\n",
    "))\n",
    "\n",
    "# Two additional ConvLSTM2D layers based on best additional_layers\n",
    "model.add(ConvLSTM2D(filters=32, kernel_size=(3, 4), activation='tanh', return_sequences=True))\n",
    "model.add(ConvLSTM2D(filters=128, kernel_size=(3, 4), activation='relu', return_sequences=False))\n",
    "\n",
    "# Flatten layer\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.4))\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Convert probabilities into binary outputs\n",
    "y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "# Compute and print performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "confusion = confusion_matrix(y_test, y_pred_binary)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "logloss = log_loss(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"Confusion Matrix: \\n{confusion}\")\n",
    "print(f\"ROC AUC: {roc_auc}\")\n",
    "print(f\"Log Loss: {logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve plot\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 8), dpi=100)\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature importance analysis with PFI and PDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_permutation_feature_importance(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Calculate permutation feature importance of a model.\n",
    "    \"\"\"\n",
    "    # Get initial accuracy\n",
    "    original_predictions = model.predict(X_test)\n",
    "    original_accuracy = accuracy_score(y_test, np.round(original_predictions))\n",
    "\n",
    "    # Initialize array to store importance\n",
    "    num_features = X_test.shape[-1]\n",
    "    importance = np.zeros(num_features)\n",
    "\n",
    "    # Permute each feature and calculate the drop in accuracy\n",
    "    for i in range(num_features):\n",
    "        X_test_permuted = X_test.copy()\n",
    "        X_test_permuted[..., i] = shuffle(X_test[..., i])\n",
    "\n",
    "        permuted_predictions = model.predict(X_test_permuted)\n",
    "        permuted_accuracy = accuracy_score(y_test, np.round(permuted_predictions))\n",
    "\n",
    "        importance[i] = original_accuracy - permuted_accuracy\n",
    "\n",
    "    return importance\n",
    "\n",
    "def plot_feature_importance(importance, feature_names):\n",
    "    \"\"\"\n",
    "    Plot permutation feature importance.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.barh(range(len(importance)), importance, align='center')\n",
    "    plt.yticks(np.arange(len(importance)), feature_names)\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title('Permutation Feature Importance')\n",
    "    plt.show()\n",
    "\n",
    "def calculate_partial_dependence(model, X_val, feature_index, num_points=100):\n",
    "    \"\"\"\n",
    "    Calculate partial dependence of a feature.\n",
    "    \"\"\"\n",
    "    feature_range = [X_val[..., feature_index].min(), X_val[..., feature_index].max()]\n",
    "    grid = np.linspace(feature_range[0], feature_range[1], num_points)\n",
    "    pdp = np.zeros(num_points)\n",
    "\n",
    "    for i, value in enumerate(grid):\n",
    "        X_val_pdp = X_val.copy()\n",
    "        X_val_pdp[..., feature_index] = value\n",
    "\n",
    "        predictions = model.predict(X_val_pdp)\n",
    "        pdp[i] = predictions.mean()\n",
    "\n",
    "    return grid, pdp\n",
    "\n",
    "def plot_partial_dependence(grid, pdp, feature_names, line_styles):\n",
    "    \"\"\"\n",
    "    Plot partial dependence of features.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, (g, p) in enumerate(zip(grid, pdp)):\n",
    "        plt.plot(g, p, label=feature_names[i], linestyle=line_styles[i])\n",
    "    plt.xlabel('Feature Value')\n",
    "    plt.ylabel('Average Prediction')\n",
    "    plt.title('Partial Dependence Plots')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Define the feature names\n",
    "channels = ['CHL', 'SST', 'BBP443', 'RS555', 'KD490', 'elevation', 'fault_distance', 'fault_CONF']\n",
    "\n",
    "# Calculate permutation feature importance\n",
    "importance = calculate_permutation_feature_importance(model, X_test, y_test)\n",
    "\n",
    "# Plot feature importance\n",
    "plot_feature_importance(importance, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot partial dependence for the first 5 channels\n",
    "grid_results = []\n",
    "pdp_results = []\n",
    "\n",
    "for i, channel in enumerate(channels[:5]):\n",
    "    grid, pdp = calculate_partial_dependence(model, X_test, i)\n",
    "    grid_results.append(grid)\n",
    "    pdp_results.append(pdp)\n",
    "\n",
    "line_styles = ['-', '--', '-.', ':', (0, (3, 5, 1, 5))]\n",
    "\n",
    "plot_partial_dependence(grid_results, pdp_results, channels[:5], line_styles)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
