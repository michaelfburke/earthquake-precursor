{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Engineering of dataset for model training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code engineers a dataset and samples of earthquake events in two specific geographic subregions. The events are represented in a set of dataframes, while the contextual data related to each event (such as environmental data) is represented in an xarray dataset.\n",
    "\n",
    "In the first section of the code, the earthquake event data is filtered to include only those events that occurred at least one week after a given minimum date. This is done to ensure that sufficient contextual data is available for each event.\n",
    "\n",
    "The function select_data_for_event extracts the date and location (latitude and longitude) of an earthquake event from the event data. It then selects the corresponding data from the xarray dataset, finding the nearest data point if the exact location is not available.\n",
    "\n",
    "The function select_grid_around_event creates a square grid around the location of an earthquake event. The grid extends by a specified size in latitude and longitude from the event location. The function also defines a time period, starting a specified number of days before the event and ending the day before the event. It then selects all data within this grid and time period from the xarray dataset, returning the data as a 4D numpy array along with the corresponding arrays of latitudes, longitudes, and dates.\n",
    "\n",
    "The function select_random_location is used to generate negative samples for the analysis. It randomly selects a location and date, making sure the location is under the sea and the date is within the range of the event data. It then checks whether there were any earthquake events within a grid around this location and within a time period around this date. If there were no events, the function selects the contextual data for this location and date as a negative sample, using the same method as select_grid_around_event.\n",
    "\n",
    "The functions generate_samples, generate_negative_samples, and combine_samples_and_labels are used to generate positive and negative samples for the analysis and combine them into a single array of samples and an array of corresponding labels. The positive samples are generated by applying select_grid_around_event to each event in the event data, and the negative samples are generated by applying select_random_location a number of times equal to the number of positive samples.\n",
    "\n",
    "In the final part of the script, the functions are applied to the event data and xarray datasets for the two subregions. The resulting samples and labels for each subregion are then combined into a single array of samples and labels for the entire analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon, LineString, MultiLineString, box\n",
    "from scipy.spatial import cKDTree\n",
    "import folium\n",
    "from folium import Rectangle\n",
    "import xarray as xr\n",
    "import random\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "MIN_DATE = '2013-01-01'\n",
    "MAX_DATE = '2023-06-01'\n",
    "BATHYMETRY_FILE = '../gebco_2022_n45.864_s29.918_w-5.947_e36.261.nc'\n",
    "# FAULTS_FILE = '../fault_data.nc'\n",
    "FAULTS_FILE = '../AFEAD_v2022.shp'\n",
    "\n",
    "grid_size = 0.5  # the spatial periphery of each sample point in degrees\n",
    "days_before = 7  # the temporal window before each sample point in days\n",
    "# MIN_DATE_OFFSET = pd.DateOffset(days=days_before)\n",
    "\n",
    "variables = ['CHL', 'analysed_sst', 'KD490', 'RRS555', 'BBP', 'elevation', 'fault_distance', 'fault_CONF'] # for full dataset\n",
    "# variables = ['CHL', 'analysed_sst'] # for testing\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# list for storing sample points\n",
    "sample_events = []\n",
    "\n",
    "SUBREGIONS = [\n",
    "    {\n",
    "        'name': 'subregion1',\n",
    "        'quake_latmin': 38.2,\n",
    "        'quake_latmax': 40.6,\n",
    "        'quake_lonmin': 24.4,\n",
    "        'quake_lonmax': 26.8,\n",
    "        'latmin': 38.2 - grid_size,\n",
    "        'latmax': 40.6 + grid_size,\n",
    "        'lonmin': 24.4 - grid_size,\n",
    "        'lonmax': 26.8 + grid_size,\n",
    "        'csv_file': 'subregion1.csv',\n",
    "        'earthquake_data': None,\n",
    "    },\n",
    "    {\n",
    "        'name': 'subregion2',\n",
    "        'quake_latmin': 35.8,\n",
    "        'quake_latmax': 38.2,\n",
    "        'quake_lonmin': 23.9,\n",
    "        'quake_lonmax': 26.3,\n",
    "        'latmin': 35.8 - grid_size,\n",
    "        'latmax': 38.2 + grid_size,\n",
    "        'lonmin': 23.9 - grid_size,\n",
    "        'lonmax': 26.3 + grid_size,\n",
    "        'csv_file': 'Subregion2_fixed.csv',\n",
    "        'earthquake_data': None,\n",
    "    },\n",
    "]\n",
    "\n",
    "COUNTRY_BORDERS = [\n",
    "    'gadm41_GRC_3.json',\n",
    "    'gadm41_TUR_2.json'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# fault line preparation functions\n",
    "\n",
    "# Function to simplify the geometry of GeoDataFrame \n",
    "def simplify_geometry(gdf, tolerance):\n",
    "    # Simplify LineStrings, keeping the topological validity of geometries intact\n",
    "    # The tolerance parameter specifies the maximum allowable offset during simplification\n",
    "    gdf['geometry'] = gdf['geometry'].simplify(tolerance)\n",
    "    return gdf\n",
    "\n",
    "# Function to filter the GeoDataFrame by the bounding box of the dataset\n",
    "def filter_by_bounding_box(gdf, ds):\n",
    "    # Define the bounding box (bbox) coordinates based on the min and max lat-lon values from the dataset\n",
    "    latmin = ds.lat.min().values\n",
    "    latmax = ds.lat.max().values\n",
    "    lonmin = ds.lon.min().values\n",
    "    lonmax = ds.lon.max().values\n",
    "\n",
    "    # Create the bbox as a GeoDataFrame\n",
    "    bbox = gpd.GeoDataFrame(geometry=[box(lonmin, latmin, lonmax, latmax)], crs=gdf.crs)\n",
    "\n",
    "    # Perform spatial join between the bbox and the gdf to retain only the geometries that intersect with the bbox\n",
    "    gdf = gpd.sjoin(gdf, bbox, op='intersects')\n",
    "    # Drop the index column added by the spatial join operation\n",
    "    gdf = gdf.drop(columns='index_right')\n",
    "\n",
    "    return gdf\n",
    "\n",
    "# Function to convert geometries into individual lat-lon points\n",
    "def get_fault_coordinates(gdf):\n",
    "    rows_list = []\n",
    "    for index, row in gdf.iterrows():\n",
    "        geom = row['geometry']\n",
    "        # If the geometry is LineString, get its coordinates directly\n",
    "        if isinstance(geom, LineString):\n",
    "            coords = geom.coords\n",
    "        # If the geometry is MultiLineString, get coordinates from each LineString it consists of\n",
    "        elif isinstance(geom, MultiLineString):\n",
    "            coords = [pt for line in geom.geoms for pt in line.coords]\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown geometry type: {geom.type}\")\n",
    "        \n",
    "        # For each coordinate point, create a new row with the original row's data and the point's lat-lon values\n",
    "        for point in coords:\n",
    "            dict1 = {}\n",
    "            dict1.update(row)  # get other columns\n",
    "            dict1.update({'lon': point[0], 'lat': point[1]})\n",
    "            rows_list.append(dict1)\n",
    "\n",
    "    # Create new DataFrame from the list of new rows and drop old geometry column\n",
    "    gdf_expanded = pd.DataFrame(rows_list)\n",
    "    gdf_expanded = gdf_expanded.drop(columns=['geometry'])\n",
    "\n",
    "    # Return the lat-lon coordinates of faults and the expanded DataFrame\n",
    "    return gdf_expanded[['lat', 'lon']].values, gdf_expanded\n",
    "\n",
    "# Function to create a KDTree from fault coordinates and query it for each point in the dataset\n",
    "def create_kdtree_and_query(ds_coords, fault_coords):\n",
    "    # Build a KDTree from fault_coords for efficient spatial queries\n",
    "    tree = cKDTree(fault_coords)\n",
    "\n",
    "    # Query the KDTree to find the closest fault to each point in the dataset\n",
    "    distances, indices = tree.query(ds_coords)\n",
    "\n",
    "    # # Store the shape of the original lat-lon data\n",
    "    # ds_shape = ds_coords.reshape(-1, 2).shape\n",
    "\n",
    "    return distances, indices\n",
    "\n",
    "# Main function to add fault information to the dataset\n",
    "def add_faults_to_dataset(ds, shapefile, tolerance=0.01):\n",
    "    # Read in the shapefile\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    # Simplify geometry and filter by bounding box\n",
    "    gdf = simplify_geometry(gdf, tolerance)\n",
    "    gdf = filter_by_bounding_box(gdf, ds)\n",
    "\n",
    "    # Get fault coordinates\n",
    "    fault_coords, gdf_expanded = get_fault_coordinates(gdf)\n",
    "\n",
    "    # Get all coordinates from the dataset\n",
    "    lats, lons = np.meshgrid(ds.lat.values, ds.lon.values)\n",
    "    ds_shape = lats.shape  # Store the shape of the original lat-lon data\n",
    "    ds_coords = np.dstack([lats, lons]).reshape(-1, 2)\n",
    "    \n",
    "    # Perform KDTree query\n",
    "    distances, indices = create_kdtree_and_query(ds_coords, fault_coords)\n",
    "\n",
    "    # Reshape the data to have the same 2D structure as the original lat-lon data\n",
    "    distances = distances.reshape(ds_shape)\n",
    "    indices = indices.reshape(ds_shape)\n",
    "\n",
    "    # Create DataArrays for distance and CONF\n",
    "    ds['fault_distance'] = xr.DataArray(distances, coords=[ds.lon, ds.lat], dims=['lon', 'lat'])\n",
    "    ds['fault_CONF'] = xr.DataArray(gdf_expanded.CONF.values[indices], coords=[ds.lon, ds.lat], dims=['lon', 'lat'])\n",
    "\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Country polygon data for the region is used to create a land mask\n",
    "def get_polygons(countries):\n",
    "    polygons = []\n",
    "    for country in countries:\n",
    "        data = gpd.read_file(country)\n",
    "        polygons.extend(data['geometry'])\n",
    "    return polygons\n",
    "\n",
    "# The land mask is used to remove earthquakes that occur on land\n",
    "def is_earthquake_on_land(point, polygons):\n",
    "    return any(point.within(polygon) for polygon in polygons)\n",
    "\n",
    "# Function to identify points that are on land\n",
    "def is_coordinate_offshore(lat, lon, polygons):\n",
    "    point = Point(lon, lat)\n",
    "    return not any(point.within(polygon) for polygon in polygons)\n",
    "\n",
    "# Function to remove earthquakes that occur on land\n",
    "def remove_land_earthquakes(df, polygons):\n",
    "    # print columns\n",
    "    print(df.columns)\n",
    "    df['point'] = [Point(xy) for xy in zip(df['longitude'], df['latitude'])]\n",
    "    df = df[~df['point'].apply(is_earthquake_on_land, args=(polygons,))]\n",
    "    return df.drop(columns='point')\n",
    "\n",
    "def add_earthquake_markers(m, df):\n",
    "    for _, row in df.iterrows():\n",
    "        popup = 'time: {time}<br>latitude: {latitude}<br>longitude: {longitude}<br>depth: {depth}'.format(**row)\n",
    "        folium.Marker([row['latitude'], row['longitude']], popup=popup).add_to(m)\n",
    "    return m\n",
    "\n",
    "# Function to add subregion borders to the map\n",
    "def add_subregion_border(m, subregion):\n",
    "    bounds = [[subregion['quake_latmin'], subregion['quake_lonmin']], [subregion['quake_latmax'], subregion['quake_lonmax']]]\n",
    "    Rectangle(bounds, color='#ff7800', fill=True, fill_color='#ffff00', fill_opacity=0.2).add_to(m)\n",
    "\n",
    "\n",
    "# Function to load data files\n",
    "def load_data_files(subregion_name):\n",
    "    chl_file = f'OCEANCOLOUR_MED_BGC_L4_MY_009_144/colour_CHL_{subregion_name}_L.nc'\n",
    "    sst_file1 = f'SST_MED_SST_L4_NRT_OBSERVATIONS_010_004/sst__{subregion_name}_L.nc'\n",
    "    sst_file2 = f'SST_MED_SST_L4_NRT_OBSERVATIONS_010_004/sst__{subregion_name}_L2.nc'\n",
    "    optics_file = f'OCEANCOLOUR_MED_BGC_L3_MY_009_143/optics_BBP443_{subregion_name}_L_refined.nc'\n",
    "    refl_file = f'OCEANCOLOUR_MED_BGC_L3_MY_009_143/refl_{subregion_name}_L_refined.nc'\n",
    "    transp_file = f'OCEANCOLOUR_MED_BGC_L3_MY_009_143/transparency_{subregion_name}_L_refined.nc'\n",
    "\n",
    "    chl = xr.open_dataset(chl_file)\n",
    "    sst = xr.concat([xr.open_dataset(sst_file1), xr.open_dataset(sst_file2)], dim='time')\n",
    "    optics = xr.open_dataset(optics_file)\n",
    "    refl = xr.open_dataset(refl_file)\n",
    "    transp = xr.open_dataset(transp_file)\n",
    "    \n",
    "    return chl, sst, optics, refl, transp\n",
    "\n",
    "# Function to merge datasets with spatial resolution and geographical area adjustment\n",
    "def merge_datasets(chl, sst, optics, refl, transp, subregion):\n",
    "    # regrid the SST dataset to match the spatial resolution and geographical area of the chlorophyll dataset\n",
    "    print('Regridding SST dataset...')\n",
    "    sst = sst.interp(lat=chl.lat, lon=chl.lon, method='linear')\n",
    "    print('Regridding optics dataset...')\n",
    "    optics = optics.interp(lat=chl.lat, lon=chl.lon, method='linear')\n",
    "    print('Regridding refl dataset...')\n",
    "    refl = refl.interp(lat=chl.lat, lon=chl.lon, method='linear')\n",
    "    print('Regridding transp dataset...')\n",
    "    transp = transp.interp(lat=chl.lat, lon=chl.lon, method='linear')\n",
    "    \n",
    "    print('Merging datasets...')\n",
    "    # merge CHL and SST data\n",
    "    ds = xr.merge([chl, sst, optics, refl, transp])\n",
    "\n",
    "    # load Bathymetry data\n",
    "    ds_bathy = xr.open_dataset(BATHYMETRY_FILE)\n",
    "    # select geographical boundaries\n",
    "    ds_bathy = ds_bathy.sel(lat=slice(subregion['latmin'], subregion['latmax']), \n",
    "                            lon=slice(subregion['lonmin'], subregion['lonmax']))\n",
    "    # regrid the bathymetry dataset to match the spatial resolution and geographical area of the chlorophyll dataset\n",
    "    print('Regridding bathymetry dataset...')\n",
    "    ds_bathy = ds_bathy.interp(lat=ds.lat, lon=ds.lon, method='linear')\n",
    "\n",
    "    print('Merging datasets...')\n",
    "    # merge bathymetry data with CHL and SST data\n",
    "    ds = xr.merge([ds, ds_bathy])\n",
    "\n",
    "    # load Fault line data\n",
    "    # ds_faults = xr.open_dataset(FAULTS_FILE)\n",
    "    # # select geographical boundaries\n",
    "    # ds_faults = ds_faults.sel(lat=slice(subregion['latmin'], subregion['latmax']), \n",
    "    #                           lon=slice(subregion['lonmin'], subregion['lonmax']))\n",
    "    # # merge fault line data with CHL, SST, and bathymetry data\n",
    "    # ds = xr.merge([ds, ds_faults])\n",
    "\n",
    "    # load and process fault line data\n",
    "    print('Processing fault line data...')\n",
    "    ds = add_faults_to_dataset(ds, FAULTS_FILE, tolerance=0.0001)\n",
    "\n",
    "    return ds\n",
    "\n",
    "# Handle missing values\n",
    "def handle_missing_values(subregion):\n",
    "    print('Handling missing values...')\n",
    "    subregion['CHL'] = subregion['CHL'].fillna(-99)\n",
    "    subregion['BBP'] = subregion['BBP'].fillna(-99)\n",
    "    subregion['KD490'] = subregion['KD490'].fillna(-99)\n",
    "    subregion['RRS555'] = subregion['RRS555'].fillna(-99)\n",
    "    subregion['analysed_sst'] = subregion['analysed_sst'].fillna(-99)\n",
    "    subregion['elevation'] = subregion['elevation'].fillna(subregion['elevation'].mean())\n",
    "    subregion['fault_distance'] = subregion['fault_distance'].fillna(subregion['fault_distance'].mean())\n",
    "    subregion['fault_CONF'] = subregion['fault_CONF'].fillna('0')\n",
    "    # subregion['fault_RATE'] = subregion['fault_RATE'].fillna('0')\n",
    "\n",
    "    return subregion\n",
    "\n",
    "# Convert CONF and RATE to numeric values\n",
    "def convert_rate_conf(subregion):\n",
    "    conf_mapping = {'A': 4, 'B': 3, 'C': 2, 'D': 1, '0': 0, '': 0}\n",
    "    rate_mapping = {'3': 3, '2': 2, '1': 1, '0': 0, '': 0}\n",
    "    \n",
    "    subregion['fault_CONF'] = xr.DataArray(pd.Series(subregion['fault_CONF'].values.flatten()).map(conf_mapping).values.reshape(subregion['fault_CONF'].shape), \n",
    "                                     coords=subregion['fault_CONF'].coords, \n",
    "                                     dims=subregion['fault_CONF'].dims)\n",
    "    \n",
    "    # subregion['fault_RATE'] = xr.DataArray(pd.Series(subregion['fault_RATE'].values.flatten()).map(rate_mapping).values.reshape(subregion['fault_RATE'].shape),\n",
    "    #                                     coords=subregion['fault_RATE'].coords,\n",
    "    #                                     dims=subregion['fault_RATE'].dims)\n",
    "    \n",
    "    return subregion\n",
    "\n",
    "# Function to format and handle earthquake data\n",
    "def handle_earthquake_data(earthquakes_subregion):\n",
    "    print('Handling earthquake data...')\n",
    "    earthquakes_subregion['date'] = earthquakes_subregion['time'].astype(str).str[:10].astype('datetime64[ns]')\n",
    "    return earthquakes_subregion\n",
    "\n",
    "def process_subregion(subregion, earthquakes_subregion):\n",
    "    chl, sst, optics, refl, transp = load_data_files(subregion['name'])\n",
    "    ds = merge_datasets(chl, sst, optics, refl, transp, subregion)\n",
    "    ds.attrs = {}\n",
    "    ds = handle_missing_values(ds)\n",
    "    ds = convert_rate_conf(ds)\n",
    "    earthquakes_subregion = handle_earthquake_data(earthquakes_subregion)\n",
    "    return ds, earthquakes_subregion\n",
    "\n",
    "\n",
    "def filter_events_by_date(earthquakes, min_date):\n",
    "    return earthquakes[earthquakes['date'] > pd.to_datetime(min_date) + pd.DateOffset(days=days_before)]\n",
    "\n",
    "\n",
    "\n",
    "def select_data_for_event(ds, event):\n",
    "    # Extract event details\n",
    "    date = pd.to_datetime(event['date'])\n",
    "    lat = event['latitude']\n",
    "    lon = event['longitude']\n",
    "\n",
    "    # Select nearest lat/lon first\n",
    "    ds_nearest = ds.sel(lat=lat, lon=lon, method='nearest')\n",
    "\n",
    "    # Extract nearest lat/lon\n",
    "    nearest_lat = ds_nearest.lat.values\n",
    "    nearest_lon = ds_nearest.lon.values\n",
    "\n",
    "    # add to samples_events\n",
    "    sample_events.append([date, nearest_lat, nearest_lon])\n",
    "\n",
    "    return nearest_lat, nearest_lon, date\n",
    "\n",
    "def select_grid_around_event(ds, event, grid_size, days_before, variables):\n",
    "    # Extract event details\n",
    "    lat, lon, date = select_data_for_event(ds, event)\n",
    "\n",
    "    # Define grid boundaries\n",
    "    lat_min = lat - grid_size\n",
    "    lat_max = lat + grid_size\n",
    "    lon_min = lon - grid_size\n",
    "    lon_max = lon + grid_size\n",
    "\n",
    "    # Define date range\n",
    "    date_min = date - pd.DateOffset(days=days_before)\n",
    "    date_max = date - pd.DateOffset(days=1)  # Exclude the day of the event itself\n",
    "\n",
    "    # Select data within grid and date range\n",
    "    ds_grid = ds.sel(\n",
    "        lat=slice(lat_min, lat_max),\n",
    "        lon=slice(lon_min, lon_max),\n",
    "        time=slice(date_min, date_max),\n",
    "    )\n",
    "\n",
    "    # Select variables\n",
    "    ds_grid = ds_grid[variables]\n",
    "\n",
    "    # Get arrays of latitudes, longitudes, and dates\n",
    "    lats = ds_grid.lat.values\n",
    "    lons = ds_grid.lon.values\n",
    "    times = ds_grid.time.values\n",
    "\n",
    "    # Get array of data values\n",
    "    values = ds_grid.to_array().transpose('time', 'lat', 'lon', 'variable').values\n",
    "\n",
    "\n",
    "\n",
    "    return values, lats, lons, times\n",
    "\n",
    "def select_random_location(ds, df_events, grid_size, days_before):\n",
    "    # Extract lat and lon values from the dataset\n",
    "    lat_values = ds.lat.values\n",
    "    lon_values = ds.lon.values\n",
    "\n",
    "    # min(lat_value)+0.5 <= lat_values <= max(lat_values)-0.5\n",
    "    # min(lon_value)+0.5 <= lon_values <= max(lon_values)-0.5\n",
    "    lat_values = lat_values[(lat_values >= lat_values.min() + grid_size) & (lat_values <= lat_values.max() - grid_size)]\n",
    "    lon_values = lon_values[(lon_values >= lon_values.min() + grid_size) & (lon_values <= lon_values.max() - grid_size)]\n",
    "\n",
    "    # Extract the date range from df_events\n",
    "    date_range = [df_events['date'].min(), df_events['date'].max()]\n",
    "\n",
    "    while True:\n",
    "        # Select a random latitude and longitude within the given ranges\n",
    "        lat = random.choice(lat_values)\n",
    "        lon = random.choice(lon_values)\n",
    "        date = random.choice(pd.date_range(*date_range))\n",
    "\n",
    "        # Define grid boundaries and date range\n",
    "        lat_min, lat_max = lat - grid_size, lat + grid_size\n",
    "        lon_min, lon_max = lon - grid_size, lon + grid_size\n",
    "        date_min = date - pd.DateOffset(days=days_before)\n",
    "        date_max = date - pd.DateOffset(days=1)\n",
    "\n",
    "        # Check if there is any earthquake event within the grid and date range\n",
    "        event_in_range = ((df_events['latitude'].between(lat_min, lat_max)) & \n",
    "                          (df_events['longitude'].between(lon_min, lon_max)) & \n",
    "                          (df_events['date'].between(date_min, date_max))).any()\n",
    "\n",
    "        # Check if lat and lon occurs at sea using is_coordinate_offshore()\n",
    "        lat_lon_offshore = is_coordinate_offshore(lat, lon, polygons=polygons)\n",
    "\n",
    "        if not event_in_range and lat_lon_offshore:\n",
    "            return select_grid_around_event(ds, {'date': date, 'latitude': lat, 'longitude': lon}, grid_size, days_before, variables)\n",
    "\n",
    "def generate_samples(ds, df_events, grid_size, days_before):\n",
    "    return [select_grid_around_event(ds, event, grid_size, days_before, variables) for event in df_events.to_dict('records')]\n",
    "\n",
    "def generate_negative_samples(ds, df_events, num_samples, grid_size, days_before):\n",
    "    return [select_random_location(ds, df_events, grid_size, days_before) for _ in range(num_samples)]\n",
    "\n",
    "def combine_samples_and_labels(positive_samples, negative_samples):\n",
    "    num_positive = len(positive_samples)\n",
    "    num_negative = len(negative_samples)\n",
    "\n",
    "    samples = positive_samples + negative_samples\n",
    "    labels = [1] * num_positive + [0] * num_negative\n",
    "\n",
    "    return samples, labels\n",
    "\n",
    "def process_samples_and_labels(ds, earthquakes, min_date, grid_size, days_before):\n",
    "    df_events = filter_events_by_date(earthquakes, min_date)\n",
    "\n",
    "    # Generate positive samples for earthquakes in the region\n",
    "    positive_samples = generate_samples(ds, df_events, grid_size, days_before)\n",
    "    print(f\"Positive samples in this region: {len(positive_samples)}\")\n",
    "\n",
    "    # Generate an equal number of negative samples for non-earthquakes\n",
    "    negative_samples = generate_negative_samples(ds, df_events, len(positive_samples), grid_size, days_before)\n",
    "    print(f\"Negative samples in this region: {len(negative_samples)}\")\n",
    "\n",
    "    # raise error if there is shape inconsistency across all samples\n",
    "    if not all([positive_samples[0][0].shape == sample[0].shape for sample in positive_samples]):\n",
    "        raise ValueError(\"Inconsistent shape across samples\")\n",
    "    \n",
    "    # Print the shape of the data arrays\n",
    "    if positive_samples:\n",
    "        print(f\"Shape of data arrays: {positive_samples[0][0].shape}\")\n",
    "    \n",
    "    return combine_samples_and_labels(positive_samples, negative_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "polygons = get_polygons(COUNTRY_BORDERS)\n",
    "\n",
    "m = folium.Map(location=[39.5, 25.5], zoom_start=6) # Initial map focused on the Aegean Sea\n",
    "\n",
    "for region in SUBREGIONS:\n",
    "    df = pd.read_csv(region['csv_file'])\n",
    "    # print(f\"Data from {region['csv_file']}:\")\n",
    "    # print(df.head())  # Prints first 5 rows of the dataframe\n",
    "    if 'longitude' in df.columns:\n",
    "        df = remove_land_earthquakes(df, polygons)\n",
    "        region['earthquake_data'] = df\n",
    "        m = add_earthquake_markers(m, df)\n",
    "        add_subregion_border(m, region)\n",
    "    else:\n",
    "        print(f\"'longitude' column not found in {region['csv_file']}\")\n",
    "    \n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Process the data for each subregion\n",
    "ds_subregion1, earthquakes_subregion1 = process_subregion(SUBREGIONS[0], SUBREGIONS[0]['earthquake_data'])\n",
    "ds_subregion2, earthquakes_subregion2 = process_subregion(SUBREGIONS[1], SUBREGIONS[1]['earthquake_data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate samples and labels for each subregion\n",
    "samples_subregion1, labels_subregion1 = process_samples_and_labels(ds_subregion1, earthquakes_subregion1, MIN_DATE, grid_size, days_before)\n",
    "samples_subregion2, labels_subregion2 = process_samples_and_labels(ds_subregion2, earthquakes_subregion2, MIN_DATE, grid_size, days_before)\n",
    "\n",
    "# Combine samples and labels for all subregions\n",
    "samples = samples_subregion1 + samples_subregion2\n",
    "labels = labels_subregion1 + labels_subregion2\n",
    "\n",
    "# Convert the lists to numpy arrays\n",
    "X = np.array(samples)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Describe the variables in the dataset\n",
    "print('Variables in subregion 1:', ds_subregion1.data_vars)\n",
    "print('Variables in subregion 2:', ds_subregion2.data_vars)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
